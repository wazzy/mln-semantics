\section{Ambiguity in word meaning}
\index{lexical ambiguity}

In order for our system to be able to make correct natural language inferences,
it must be able to handle paraphrasing.  For example, in order to license the
entailment pair in (\ref{ex:syn-hyp-pos}), the system must recognize that
``owns'' is a valid paraphrase for ``has'', and that a ``car'' is type of
``vehicle'':

\entpairex{ex:syn-hyp-pos}{Ed owns a car.}{Ed has a vehicle.}

We address this problem as described in Section~\ref{sec:interface}: 
We use distributional information to
generate inferences stating, for example,  that ``has'' can be substituted for
``owns''. This inference is weighted by the degree to which 
``owns'', in the context in which it is used in
(\ref{ex:syn-hyp-pos}), is similar to ``has''. To integrate these
inference rules with the logical form representations of sentences
like (\ref{ex:syn-hyp-pos}), we use the formalism introduced in
Section \ref{sec:interface}. We now describe how we instantiate it in
the current paper.

% KE: weird paragraph, sounds like we had not introduced the notion
% of inference rules based on distributional information
% before. Instead, point to section 3. 
% Perhaps the most natural fit for our system of projecting distributional
% information into logical forms is trying to generate inference rules to address
% lexical ambiguity.  For any natural language sentence $A$, a word $v$ in $A$ may
% be replaced by a synonym of $v$, $w$, resulting in a new sentence $A'$.  The
% degree to which $A'$ means the same thing as $A$ is determined by how well $w$
% fits the context of $v$ in $A$.  Thus, in order to capture the
% strength of our conviction that
% $A$ entails $A'$, we want to generate an inference rule stating that $v$ implies
% $w$ to the degree that that $w$ fits the context of $v$.


First, we generate a vector space $V$.  We have chosen to implement a very
simple vector space based on a bag-of-words representation of context.  To ensure that the entries in the vector
space correspond to the predicates in our logical forms, we first lemmatize all
sentences in our corpus using the same lemmatization process as Boxer.
The features used by $V$ are the $N$ most frequent lemmas, excluding stopwords.  
To calculate the vector in $V$ for a lemma, we count the number of times the lemma appears in the same sentence
as each feature, and then calculate the point-wise mutual information (PMI)
between the lemma and each feature.  The resulting PMI values for each feature
are used as the vector for the lemma.

As the \emph{similarity function} \simfunc on our vector space, we use cosine
similarity. For two vectors $\vec v$ and $\vec w$, their similarity is 
\[ \simfunc(\vec v, \vec w) = cosine(\vec v, \vec w) = \frac{\vec v \cdot \vec
w}{\|\vec v\|~\|\vec w\|}\]

Logical forms in our system are generated by Boxer, so our logical language
\loglang is the set of formulas that may be returned from Boxer (modulo
some modifications described in Section~\ref{sec:implicativity}).  Likewise, the
set of predicate symbols $\predsym{\loglang}$ are the predicates generated by
Boxer. Boxer's predicates, as represented by the {\tt pred} relation in Boxer's
Prolog output,\footnote{See
\url{http://svn.ask.it.usyd.edu.au/trac/candc/wiki/DRSs} for the detailed
grammar of Boxer DRS output.} consist of a word lemma and a token index
indicating the original token that generated that predicate.  
% lexical mapping maps to vectors, not words!
Our \emph{lexical mapping} function maps each predicate symbol to the
vector that represents the lemma portion of the predicate.

In order to assess the similarity between a word's context and a possible
replacement word, we must define a \textit{context mapping} that generates a
context from a predicate $P \in \predsym{\loglang}$ and a formula $G \in
\loglang$.  For the current paper we use the simplest possible
definition for $\kappa$, which ignores semantic relations. We define 
% We can define our context mapping as a function that maps $P$ to the set of
% vectors representing the other predicates in $G$ along with the relations that
% connect them.
% \begin{align*}
% \kappa(P,G) = \{ (r_i, \ell(Q)) ~| 
% &~Q \text{ is a predicate found in } G, \\
% &~r_i = \text{the relation connecting } P \text{ and } Q, \text{ and } \\
% &~Q \neq P \}
% \end{align*}
% However, in our current scenario, the only ``relation'' we use is the relation
% of being in the same sentence.  So, we are defining 
the context of $P$ as the
vectors of all predicates $Q$ that occur in the same sentence as $P$.
Since every predicate in a logical form returned by Boxer is indexed
with the sentence from which it was generated, we can define a simple context
mapping that defines a predicate's context solely in terms of the other
predicates generated by Boxer for that sentence.
\begin{align*}
\kappa(P,G) = \{ (same\text{-}sentence, \ell(Q)) ~|
&~Q \text{ is a predicate found in } G, \\
&~Q\text{'s sentence index} = P\text{'s sentence index}, \text{ and } \\
&~Q \neq P \}
\end{align*}
Note that the only predicates $Q$ that are used are those derived from the
lemmas of words found in the text.  Meta-predicates representing relations such
as $agent$, $patient$, and $theme$ are not included.

The context mapping $\kappa$ computes a context for a predicate $P$
occurring in a formula $G$. Next we require a 
\textit{contextualization function} that uses the context returned by
$\kappa$ to compute a context-specific vector for $P$. Again we use
the simplest instantiation possible. Our contextualization
function just computes the sum of the vectors for each lemma in the context \[ \alpha(\vec v,
c) = \sum_{(r_i, \vec w_i) \in c} \vec w_i \]  Other, more complex
instantiations of $\kappa$ and $\alpha$ are possible. We comment on
this further in Section~\ref{sec:future}. 

Based on these definitions, we compute the \textit{contextualized
inference projection} $\Pi^G_{\simfunc,\zeta,\ell}(P)$, the set of weighted
inference rules mapping predicate $P$ to its potential replacements,
as described in Section~\ref{sec:interface}.

Finally, in order to limit the number of inference rules generated in the
inference projection, we define a restriction function $\zeta$ that specifies,
for a predicate $P \in \predsym{\loglang}^n$, which of the predicates in
$\predsym{\loglang}^n$ may serve as replacements.  Our system uses WordNet
\citep{miller:wordnet2009} to restrict substitutions only to those predicates
representing synonyms or hypernyms of the lemma underlying $P$.  So, for a
predicate $P \in \predsym{\loglang}^n$ and a set of predicates ${\cal Q}
\subseteq \predsym{\loglang}^n$, we define $\zeta$ as \[ \zeta(P,{\cal Q}) = \{
Q \in {\cal Q} ~|~ Q\text{'s lemma is a synonym of, a hypernym of, or equal to
}P\text{'s} \} \]


\subsection*{A lexical ambiguity example}

Assume we have sentence \eqref{ex:lexical-ambiguity}, which is parsed by C\&C
and translated into DRT by Boxer, as shown in Figure
\ref{drs:lexical-ambiguity}.

\vspace{5mm}
\begin{covex}\label{ex:lexical-ambiguity}
\begin{itemize} \itemsep -3pt
  \item[{\it p:}]~~~  A stadium craze is sweeping the country.
  \item[{\it h1:}]~~~~A craze is covering the nation.
  \item[{\it h2*:}]~~~~A craze is brushing the nation.
\end{itemize}
\end{covex}

\begin{figure}
  \centering
  ~~~~~~~~
  \subfloat[Dependency output from C\&C]{\label{drs:lexical-ambiguity-deps}
    \begin{minipage}[c][0.7\width]{0.5\textwidth}
	  %\centering
	    \begin{tikzpicture}[level distance=50pt, sibling distance=30pt]
	      \Tree 
	        [.sweep
	          \edge node[auto=right]{ncsubj}; [.craze  
	            \edge node[auto=right]{det};   [.a ]
	            \edge node[auto=left]{ncmod}; [.stadium ]
	          ]
	          \edge node[auto=right]{aux}; [.is ]
	          \edge node[auto=left]{dobj}; [.country 
	            \edge node[auto=left]{det}; [.the ]
	          ]
	        ]
	    \end{tikzpicture}
		% (ncmod _ craze_2 stadium_1)
		% (det craze_2 A_0)
		% (det country_6 the_5)
		% (dobj sweeping_4 country_6)
		% (aux sweeping_4 is_3)
		% (ncsubj sweeping_4 craze_2 _)
	\end{minipage}
  }
  \subfloat[DRT output from Boxer]{\label{drs:lexical-ambiguity-drs}
    \begin{minipage}[c][0.7\width]{0.5\textwidth}
	  %\centering
		\drs{~x0 x1 e2 x3~}{
		  ~\pred{stadium}{1002}(x0)~ \\
		  ~nn(x0, x1)~ \\
		  ~\pred{craze}{1003}(x1)~ \\
		  ~agent(e2, x1)~ \\
		  ~\pred{sweep}{1005}(e2)~ \\
		  ~event(e2)~ \\
		  ~\pred{country}{1007}(x3)~ \\
		  ~patient(e2, x3)~
		}
	\end{minipage}
  }
  \caption{Dependency parse tree and DRT interpretation of
  the premise in \eqref{ex:lexical-ambiguity}}
  \label{drs:lexical-ambiguity}
\end{figure}

The DRS in Figure \ref{drs:lexical-ambiguity-drs}, a formula of logical language
\loglang, shall be denoted by $G$.  Formula $G$ contains a unary predicate
$sweep_{1005}$.  In order to generate weighted substitution rules for
$sweep_{1005}$, we calculate the {\it contextualized inference projection} of
$sweep_{1005}$: the set of inference rules mapping $sweep_{1005}$ to each
(unary) predicate $Q \in \predsym{\loglang}^1$, with each rule weighted by the
similarity of the vector representing the context of $sweep_{1005}$ in $G$ to
the vector representing the replacement $Q$. This is 
\begin{align*}
&\Pi^G_{\simfunc, \zeta, \ell}(sweep_{1005}) = \\
& \hspace{50px} \{ (F, \eta) \mid \exists Q \in \zeta(P, \predsym{\loglang}^1)~[ \\
& \hspace{110px} F = \forall x.[sweep_{1005}(x) \to Q(x)] \text{ and } \\
& \hspace{110px} \eta = \simfunc\big(\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G)), \ell(Q)\big)~] \}
\end{align*}

Let us assume that our logical language \loglang also includes unary predicates
$cover_{2004}$ and $brush_{3004}$ and that the lemmas {\it cover} and {\it
brush} are known to be synonyms of {\it sweep} (though from different senses). 
In other words, \[ \{ cover_{2004},~ brush_{3004} \} \in \zeta(sweep_{1005},~
\predsym{\loglang}^1) \] So, in the calculation of $\Pi^G_{\simfunc,\zeta
\ell}(sweep_{1005})$, we will generate weighted inference rules $(F,\eta)$ for
both $cover_{2004}$ and $brush_{3004}$.  This will allow us to calculate the
probability of inference for both hypotheses in \eqref{ex:lexical-ambiguity}.

\vspace{8mm}
We look first at $cover_{2004}$.  The rule formula $F$ is instantiated simply as
\[ \forall x.[sweep_{1005}(x) \to cover_{2004}(x)] \]  The weight $\eta$ is the
similarity between the context of $sweep_{1005}$ in $G$, and $cover_{2004}$.
The context vector for $sweep_{1005}$ is calculated as \[
\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G)) \]  Since we defined the
lexical mapping $\ell(P)$ to simply return the vector from $V$ for the lemma
portion of the predicate $P$, $\ell(sweep_{1005}) = \vv{sweep}$ and
$\ell(cover_{2004}) = \vv{cover}$.  

The context of $P$ in $G$, $\kappa(P,G)$ is the set of a set of
predicates and their relations to $P$, so
\begin{align*}
\kappa(sweep_{1005}, G) 
= \{ & (\ell(\pred{stadium}{1002}),~same\text{-}sentence) \} \\
     & (\ell(\pred{craze}{1003}),~same\text{-}sentence), \\ 
     & (\ell(\pred{country}{1007}),~same\text{-}sentence), \\ 
= \{ & (\vv{stadium},~same\text{-}sentence), \\
     & (\vv{craze},~same\text{-}sentence), \\
     & (\vv{country},~same\text{-}sentence) \}
\end{align*}
We defined our contextualization function $\alpha(\vec v, c)$ to be the vector
sum of word vectors from the context $c$, so
\begin{align*}
\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G))
& = \alpha(\vv{sweep},~ \{ 
                (\vv{stadium},~same\text{-}sentence), \\
& \hspace{60px} (\vv{craze},~same\text{-}sentence), \\
& \hspace{60px} (\vv{country},~same\text{-}sentence) \}) \\
& = \vv{stadium} + \vv{craze} + \vv{country}
\end{align*}

Finally, since we have the vector representing the context of $sweep_{1005}$ in
$G$ and the vector representing the replacement predicate $cover_{2004}$, we can
compute the weight, $\eta$ for our inference rule $\forall x.[sweep_{1005}(x)
\to cover_{2004}(x)]$ as
\begin{align*}
& \simfunc\big(\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G)), \ell(Q)\big) \\
& \hspace{120px} = \simfunc\big(\vv{stadium} + \vv{craze} + \vv{country},~ \vv{cover}\big) \\
& \hspace{120px} = cosine\big(\vv{stadium} + \vv{craze} + \vv{country},~ \vv{cover}\big)
\end{align*}
Likewise, the rule for replacing $sweep_{1005}$ by $brush_{3004}$ would be 
$\forall x.[sweep_{1005}(x)$ $\to$ $brush_{3004}(x)]$ weighted by 
$cosine\big(\vv{stadium} + \vv{craze} + \vv{country},~ \vv{brush}\big)$.

Since, $cosine\big(\vv{stadium}$ $+$ $\vv{craze}$ $+$ $\vv{country},~
\vv{cover}\big)$ $>$ $cosine\big(\vv{stadium}$ $+$ $\vv{craze}$ $+$ $\vv{country},~
\vv{brush}\big)$, {\it cover} is considered to be a better replacement for {\it
sweep} than {\it brush} in the sentence ``A stadium craze is sweeping the
country''.  Thus, the rule $\forall x.[sweep_{1005}(x) \to cover_{2004}(x)]$
will be given more consideration during inference, and hypothesis {\it h1} will
be determined to be more probable than {\it h2}.




\subsection*{Hypernymy}

According to our definition of $\zeta$ above, we construct inference rules of
the form $\forall x_1, \ldots, x_n[ P(x_1, \ldots, x_n) \to Q(x_1, \ldots x_n)
]$ where $Q$ is a synonym or hypernym of $P$.  Thus, for two synonyms $A$ and
$B$, we will generate rules $A \to B$ and $B \to A$.  However, for hypernym
relationships, we only construct the inference rule entailing {\it up} the
hierarchy: from the hyponym to the hypernym.  This is important for licensing
correct inferences.  Consider example \eqref{ex:hyp-1}.
\entpairex{ex:hyp-1}{Ed owns a car.}{Ed has a vehicle.}
Here the inference is valid since a {\it car} is a type of {\it vehicle}. 
For this pair, our system will generate the rule $\forall x[car(x) \to
vehicle(x)]$ and assign a weight based on the similarity of the lemma $vehicle$
to the context of $car$ in the premise sentence.  However, an inference in the
reverse direction of \eqref{ex:hyp-1} would be invalid, which is why we do not
generate the reverse inference rule.


%\subsection*{Integration between logical and distributional phenomena}

With hypernymy, we can see how our system naturally integrates logical phenomena
with distributional information.  In example \eqref{ex:hyp-1}, the
distributional similarity between {\it vehicle} and the context of {\it car}
affects the overall probability of inference for the pair.  However, it does not
override the logical requirements imposed by the hypernym relationship: if the
premise and hypothesis were reversed then it would not matter how similar the
words were since the inference would be impossible.

The logical rules generated for hypernyms work properly with other logical
aspects as well.  For example, in \eqref{ex:hyp-3} below we can see that the
direction of entailment along the hypernym hierarchy is reversed when the words
appear in negative contexts.  Our system handles this correctly.

% Above, we stated that the entailment in \eqref{ex:hyp-1} was licensed because a
% {\it car} is a type of {\it vehicle} and we can entail from a subset to a
% superset.  In fact, the situation is complicated a bit because the direction of
% entailment is actually dictated by the polarity of the context in which the
% words appear.
% 
% Consider example \eqref{ex:hyp-3} below
\entpairex{ex:hyp-3}{Ed does not own a vehicle.}{Ed does not have a car.}
% This entailment is valid despite the fact that we are entailing from {\it
% vehicle} to {\it car}, the opposite direction as in example \eqref{ex:hyp-1}.
% The difference is that in \eqref{ex:hyp-1}, the words appeared in a {\it
% positive context} while in \eqref{ex:hyp-3} they appear in a {\it
% negative context} since they are embedded under a single negation.
% 
% However, the approach that we have chosen to take handles this interaction
% naturally.  The softened inference rules we generate for hypernym relationships
% may lower the probability of entailment versus a similar hard rule (when the
% weight is less than 1), but if an entailment rule does not fit the polarity of
% the context, then it will not raise the probability of entailment.
% 
% In addition to negation, other linguistic constructs such as quantifiers and
% implicative verbs may affect the polarity of a context
% \citep{maccartney:iwcs2009}.  Our system handles all equally well.
