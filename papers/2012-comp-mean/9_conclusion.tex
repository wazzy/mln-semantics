\section{Future work}
\label{sec:future}

Our plans for continued work can be divided into two categories: work on the
theoretical side and work on implementation and evaluation.

From a theoretical perspective, we have used a simplistic bag-of-words
approach for
computing a context-specific vector for a predicate based on its
formula context (functions $\alpha$ and $\kappa$). We plan to move to a more informative construction
that takes semantic relations into account. This will be interesting in particular because the relations that can be read off a logical
form differ from those available in a dependency parse. For example,
we can check whether two predicates occur within the same DRS, or
whether they apply to a common variable. We can also ask what 
influence different logical connectives have on perceived word meaning. 

%Additionally, up to this point we have only addressed word-level paraphrasing 
%with weighted lexical ambiguity rules that connect individual words.  However, 
%our framework could easily be extended to allow for weighted paraphrase rules
%for higher-order phrases such as noun-noun compounds, adjective-noun compounds,
%or full noun phrases.

We would also like to extend our formalism to address a wider range of
linguistic phenomena.  Many phenomena are better described using weights than
through categorial analyses, and first-order representations do not correctly
address this.  By extending our framework, we hope to be able to apply weights
derived from distributional information to a wide variety of modeled concepts.
The inference rules generated by our approach to factivity might be good
candidates for this extension.  \citet{nairn:icos2006} proposed that there may  
be ``degrees of factivity'' based on the context of the verb.
Because the inference rules that we use to
activate the presupposition triggers are externalized, they can be weighted
independently of the rest of the semantic analysis.  Right now the rules are
either generated or not, which is equivalent to assigning a weight of either 1
or 0, but a weighted approach could be taken instead.

% KE I find even this speculation too much, since I am not sure I
% believe that we can find a distributional approach to this problem. 
% I would need to see data first.
%
% if the degree to which an implicative is
% presuppositional depends on context, then perhaps these weights could be
% generated from a distributional vector space as well.

% [START Weighting implicative inference rules]
% 
% One important result arising from our approach to implicativity is that
% either the positive or negative entailment of a nested proposition is determined
% by the externalized rules, such as those in Figure \ref{drs:impl-3-rules}, that
% are used to activate the presupposition triggers of that proposition. 
% Previously, we discussed how the appropriate rules may be either added or not
% added depending on the implication signature for the verb.  However, it is
% possible for some verbs that the implication signature may, in fact, be somewhat
% fuzzy.  For example, the implicativity of a verb may be related to the
% distributional context in which it appears.  
% 
% Because our system has externalized implicativity rules, we can assign
% independent weights according to what the verb's context says about its
% implications.  If a verb predicate $P$ in formula $G$ occurs in a positive
% context, and our distributional information tells us that that verb, according
% to its context $\kappa(P,G)$, is negatively entailing in positive contexts with
% confidence $\eta$, the we can generate the weighted rule:
% \[ (\forall~ p.[~ \exists~ e.[~ (P(e) \land theme(e,p)) ~] \to \NEGPRED{p}
% ~],~~\eta) \]
% Under the view, perfect confidence that  $P$ was negatively entailing would
% correspond to a weight $\eta=1$ and perfect confidence that $P$ was {\it not}
% negatively entailing would correspond to a weight $\eta=0$.  Thus, our previous
% description of a rule either being added or not added according to the
% implication signature can be seen as a subset of this technique where the only
% choices for $\eta$ are 0 and 1.
% 
% While we do not have a scheme for generate such weights at this time, our
% framework supports this addition.
% 
% [END]


% [think about vector spaces that take logical form into account similar to how
% pado and lapata took dependencies into account.]

From an implementation perspective, we would like to run a large-scale
evaluation of our techniques.
However, the major barrier to scaling up is that the Alchemy software has severe
inefficiencies in terms of memory requirements and speed.  This prevents us
from executing larger and more complex examples.  There is on-going work to improve
Alchemy \citep{gogate:uai2011}, so we hope to be able to make use of new
probabilistic inference tools as they become available.

% KE: let's not go there yet. This would take too much time to explain.
% We are also interested in comparing our approaches to recent work on building
% vectors from entire sentences [TODO: CITATIONS].  We would like to example
% how our approach compares in the task of measuring similarity between
% phrases.


\section{Conclusion}

In this paper, we have defined a link between logical form and vector
spaces through a lexical mapping of predicate symbols to points in
space. We address polysemy not through separate predicate symbols for
different senses of a word, but by using a single predicate symbol
with a lexical mapping that gets
adapted to the context in which the predicate symbol appears. We use
the link to project weighted inferences from the vector space to the
logical form. 

We showed how these weighted first-order
representations can be used to perform probabilistic first-order inferences
using Markov Logic.  We have shown how our approach handles three distinct
phenomena, word meaning ambiguity, hypernymy, and implicativity, as well as
allowing them to interact appropriately.  Most importantly our approach allows 
us to model some phenomena with hard first-order techniques and
other phenomena with soft weights, and to do all of this within a
single, unified framework.
The resulting approach is able to correctly solve a number of difficult
textual entailment problems that require handling complex combinations of these
important semantic phenomena.

% The framework we have developed takes a pair of natural
% language sentences as input and parses them into DRS \citep{kamp:book93}
% representations.  It then augments those representations by linking the
% predicates back to the original words in the sentences and incorporating
% coreference information from OntoNotes \citep{hovy:naacl2006}.
% Since DRSs are hierarchical structures, our approach flattens them to a simple
% list of atoms while keeping track of the original structure through DRS labels
% as arguments, allowing inferences to be performed in 
% an MLN. In order to maintain the DRS semantics in the flat logical form, we have 
% hand-written a collection of inference rules that are used in the inference. 
% We also generate a list of inference rules to address the particular
% linguistic phenomena that we are handling.  Categorial rules based
% on implication signatures \citep{nairn:icos2006} are used to handle
% implicativity and factivity.  Weighted rules are used to address issues of
% word meaning in context.



\section*{Acknowledgements}

This work was supported by the U.S. Department of Defense through a National
Defense Science and Engineering Graduate Fellowship (NDSEG) Fellowship for the
first author, National Science Foundation grant IIS-0845925 for the second
author, Air Force Contract FA8750-09-C-0172 under the DARPA Machine Reading
Program and NSF grants IIS-0712097 and IIS-1016312 for the third author, and a
grant from the Texas Advanced Computing Center (TACC), The University of Texas
at Austin.

We would like to thank Gemma Boleda, Louise McNally, Karl Pichotta, and Yinon 
Bentor for their helpful comments.
