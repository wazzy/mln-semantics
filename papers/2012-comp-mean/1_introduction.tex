\section{Introduction}

Logic-based \index{logical semantics} 
representations of natural language meaning have a long history
\citep{montague:tj1970,kamp:book93}\nocite{thomason:book1974}.
Representing the meaning of language in a first-order logical form is appealing
because it provides a powerful and flexible way to express even complex
propositions. However, systems built solely using first-order logical forms tend
to be very brittle as they have no way of integrating uncertain knowledge.
They, therefore, tend to have high precision at the cost of low recall
\citep{bos:emnlp2005}.

Recent advances in computational linguistics have yielded robust methods that
use statistically-driven weighted models.  For example, distributional
\index{distributional semantics}
models of word meaning have been used successfully to judge paraphrase
appropriateness by representing the meaning of a word in context as a point in a
high-dimensional semantics space
\citep{erk:emnlp2008,thater:acl2010,reisinger:naacl2010,dinu:emnlp2010,vandecruys:emnlp2011}.
However, these models only address word meaning, and do not
address the question of providing meaning representations for complete
sentences. It is a long-standing open question how best to
integrate the weighted or probabilistic information coming from such modules
with logic-based representations in a way that allows for reasoning over both. 
See, for example, \citet{hobbs:alj93}.

The goal of this work is to establish a formal system for combining
logic-based meaning representations with weighted information into a single
unified framework.  This will allow us to obtain the best of both situations: we
will have the full expressivity of first-order logic and be able to reason with
probabilities.  We believe that this will allow for a more complete and robust
approach to natural language understanding.

While this is a large and complex task, this paper proposes first steps toward
our goal by presenting a mechanism for injecting distributional word-similarity
information from a vector space into a first-order logical form.  We
define a mapping from predicate symbols of logical form to points in
vector space. Our main aim in linking logical form to a vector
 space in this paper is to project inferences from the vector space to
logical form. The inference rules that we use are based on
substitutability. In a suitably constructed distributional
representation, distributional similarity between two words or
expressions $A$ and $B$ indicates
that $B$ can be substituted for $A$ in text~\citep{lin:nlej2001}. This can be described through an inference rule $A \to B$. Distributional
information can also be used to determine the degree $\eta$ to which the rule applies in a
given sentence context
\citep{SzpektorEtAl:08,mitchell:acl2008,erk:emnlp2008,thater:acl2010,reisinger:naacl2010,dinu:emnlp2010,vandecruys:emnlp2011}. This
degree $\eta$ can be used as a weight on the inference rule $A\to B$. 

In this paper, we first present our formal framework for projecting inferences
from vector space to logical form.  We then show how that framework can be
applied to a real logical language and vector space to address issues of
ambiguity in word meaning.  Finally, we show how the weighted inference rules
produced by our approach interact appropriately with the first-order logical
form to produce correct inferences.

Our implementation uses Markov Logic Networks (MLN) \citep{richardson:mlj06} as
the underlying engine for probabilistic inference.  We are able to demonstrate
that an MLN is able to properly integrate the first-order logical representation
and weighted inference rules so that inferences involving correct word sense are
assessed as being highly probable, inferences involving incorrect word sense are
determined to be low probability, and inferences that violate hard logical rules
are determined to have the lowest probability.
