\section{Background}

\textbf{Textual entailment.}
\index{Recognizing Textual Entailment}
Recognizing Textual Entailment (RTE) is the task of determining whether one
natural language text, the {\it premise}, implies another, the {\it hypothesis}.
For evaluation of our system, we have chosen to use a variation on RTE in which
we assess the relative probability of entailment for a of set of hypotheses.

We have chosen textual entailment as the mode of evaluation
for our approach because it offers a good framework for testing whether a system
performs correct analyses and thus draws the right inferences from a given text.
As an example, consider \eqref{ex:backgound-rte} below.    
\begin{covex}\label{ex:backgound-rte}
\begin{itemize} \itemsep -3pt
  \item[{\it p:}]~~~    The spill left a stain.
  \item[{\it h1:}]~~~~The spill resulted in a stain.
  \item[{\it h2*:}]~~~~The spill fled a stain.
  \item[{\it h3*:}]~~~~The spill did not result in a stain.
\end{itemize}
\end{covex}
Here, hypothesis {\it h1} is a valid entailment, and should be judged to have
high probability by the system.  Hypothesis {\it h2} should have lower probability since
it uses the wrong sense of {\it leave} and {\it h3} should be low probability
because the logical operator $not$ has reversed the meaning of the premise
statement.

% For example, to test whether a system correctly handles implicative verbs, one
% can use the \emph{premise} $p$ along with the \emph{hypothesis} $h$ in
% \eqref{ex:imp-fact-nested} below. If the system analyses the two sentences
% correctly, it should infer that $h$ holds.
While the most prominent forum using textual entailment is the Recognizing
Textual Entailment (RTE) challenge \citep{dagan:rte2005}, the RTE datasets do
not test the phenomena in which we are interested. For example, in order to
evaluate our system's ability to determine word meaning in context, the RTE pair
would have to specifically test word sense confusion by having a word's context
in the hypothesis be different from the context of the premise.  However, this
simply does not occur in the RTE corpora.  In order to properly test our
phenomena, we construct hand-tailored premises and hypotheses based on
real-world texts.

\vspace{2mm}
\textbf{Logic-based semantics.}
\index{logical semantics}
\index{Discourse Representation Theory}
Boxer \citep{bos:coling2004} is a software package for wide-coverage semantic
analysis that provides semantic representations in the form of Discourse
Representation Structures \citep{kamp:book93}. It builds on the C\&C CCG parser
\citep{clark:acl04}.

\citet{bos:emnlp2005} describe a system for Recognizing Textual Entailment
(RTE) that uses Boxer to convert both the premise and hypothesis of an RTE pair
into first-order logical semantic representations and then uses a theorem prover
to check for logical entailment. 
% \citet{bos:trec2006} varies this model in order
% to use Boxer in a question answering setting by using Boxer to generate a
% logical representation of a document and a question and attempting to unify the
% two to find an answer to the question.


\vspace{2mm}
\noindent\textbf{Distributional models for lexical meaning.} 
\index{distributional semantics} Distributional
models describe the meaning of a word through the context in which it
appears~\citep{landauer97:solution,lund96:producing}, where contexts can be
documents, other words, or snippets of syntactic structure. Based on
the hypothesis that words that are similar in meaning will occur in
similar contexts~\citep{harris:wj1954,firth:slaj1957}, distributional
models predict semantic similarity between words based on
distributional similarity. They can be learned in an unsupervised fashion.
Recently distributional models have been used to predict the applicability of
paraphrases in context \citep{erk:emnlp2008,thater:acl2010,reisinger:naacl2010,dinu:emnlp2010,vandecruys:emnlp2011}.
For example, in ``The spill left a stain'', {\it result in} is a better
paraphrase for {\it leave} than {\it flee}, because of the context of {\it spill}
and {\it stain}.  In the sentence ``The suspect left the country'', the
opposite is true: {\it flee} is a better paraphrase. Usually, the distributional
representation for a word mixes all its usages (senses). For the paraphrase
appropriateness task, these representations are then reweighted, extended, or
filtered to focus on contextually appropriate usages.


\vspace{2mm}
\noindent\textbf{Markov Logic.} 
\index{Markov logic}

In order to perform logical inference with weights, we draw
from the large and active body of work related to Statistical Relational AI
\citep{getoor:book2007}.  Specifically, we make use of Markov Logic Networks
(MLNs) \citep{richardson:mlj06} which employ weighted graphical models to
represent first-order logical formulas. MLNs are appropriate for our approach
because they provide an elegant method of assigning weights to first-order
logical rules, combining a diverse set of inference rules, and performing
probabilistic inference. \index{probabilistic inference}

An MLN consists of a set of weighted first-order clauses.  It provides a way of
softening first-order logic by making situations in which not all clauses are
satisfied less likely, but not impossible \citep{richardson:mlj06}. More
formally, if $X$ is the set of all propositions describing a world (i.e. the
set of all ground atoms), $\mathcal{F}$ is the set of all clauses in the MLN,
$w_i$ is the weight associated with clause $f_i \in \mathcal{F}$,
$\mathcal{G}_{f_i}$ is the set of all possible groundings of clause $f_i$, and
$\mathcal{Z}$ is the normalization constant, then the probability of a
particular truth assignment $\mathbf{x}$ to the variables in $X$ is defined as:
\[ P(X = \mathbf{x}) = \frac{1}{\mathcal{Z}} \exp\left(\sum_{f_i \in
\mathcal{F}} w_i \sum_{g \in \mathcal{G}_{f_i}}g(\mathbf{x}) \right) =
\frac{1}{\mathcal{Z}} \exp\left(\sum_{f_i \in \mathcal{F}} w_i n_i(\mathbf{x})
\right) \] where $g(\mathbf{x})$ is 1 if $g$ is satisfied and
0 otherwise, and $n_i(\mathbf{x})= \sum_{g\in \mathcal{G}_{f_i}}g(\mathbf{x})$
is the number of groundings of $f_i$ that are satisfied given the current truth
assignment to the variables in $X$. This means that the probability of a truth
assignment rises exponentially with the number of groundings that are satisfied.

Markov Logic has been used previously in other NLP applications
(e.g. \citet{poon:emnlp2009}).  However, this paper differs in that it is an
attempt to represent deep logical semantics in an MLN.

While it is possible to learn rule weights in an MLN directly from training data,
our approach at this time focuses on incorporating weights computed
by external knowledge sources.  Weights for word meaning rules are computed from
the distributional model of lexical meaning and then injected into the MLN. 
Rules governing implicativity are given infinite weight (hard constraints).

We use the open source software package Alchemy \citep{kok:tr05} to perform MLN
inference.
