% logical language
\newcommand{\loglang}{\ensuremath{{\cal{L}}}\xspace}
% predicate symbols
\newcommand{\predsym}[1]{\ensuremath{{\cal{P}}_{#1}}\xspace}
% similarity function
\newcommand{\simfunc}{\ensuremath{\mathrm{sim}}\xspace}

\section{Linking logical form and vector spaces}
\label{sec:interface}
\index{vector space}

In this section we define a link between logical form and vector space
representations through a mapping function that connects predicates in logical
form to points in vector space. \citet{gardenfors:book2004} uses the
interpretation function for this purpose, such that logical formulas are
interpreted over vector space representations. However, the 
\emph{conceptual spaces} that he uses are not distributional. Their
dimensions are qualities, like the hue and saturation of a color or
the taste of 
a fruit. Points in a conceptual space are, therefore, potential entities. In
contrast, the vector spaces that we use are distributional in nature, and, therefore, cannot be interpreted as potential
entities. A point in such a space is a potential word, defined through
its observed contexts. For
this reason, we define the link between logical form and vector space through a
second mapping function independent of the interpretation function, which we
call the \emph{lexical mapping} function.

\subsection*{Lexical mapping and inference projection} 

% [TODO: \\vec is making vectors bold instead of using an overarrow]
% KE: leave them bold. this is due to mathptmx.
% Probably too much of a nuisance to change back, plus it 
% might be inconsistent with other papers in the book to have an overarrow.

Let $V$ be a vector space whose dimensions stand for elements of  textual
context. We also write $V$ for the set of points in the space. We assume that
each word is represented as a point in vector space.\footnote{The assumption of
a single vector per word is made for the sake of simplicity. If we want to cover
models in which each word is represented through multiple
vectors~\citep{reisinger:naacl2010,dinu:emnlp2010}, this can be done through
straightforward extensions of the definitions given here.}
The central relation in vector spaces is semantic similarity. We represent this
through a \textit{similarity function} \[\simfunc: V \times V \to [0,1] \] that
maps each pair of points in vector space to their degree of similarity. While
most similarity functions in the literature are symmetric, such that 
$\simfunc(\vec v, \vec w) = \simfunc(\vec w, \vec v)$, our definition also
accommodates asymmetric similarity measures like \citet{kotlerman:nlej2010}.

We link logical form and a vector space through a function that maps every
predicate symbol to a point in space. Let \loglang be a logical language. For
each $n \ge 0$, let the set of $n$-ary predicate symbols of \loglang be
$\predsym{\loglang}^n$, and let $\predsym{\loglang} = \cup_{n \ge 0}
\predsym{\loglang}^n$. Let $V$ be a vector space. Then a \emph{lexical mapping
function} from \loglang to $V$ is a function $\ell:
\predsym{\loglang} \to V$.



A central property of distributional vector spaces is that they can predict
similarity in meaning based on similarity in observed
contexts~\citep{harris:wj1954}. \citet{lin:nlej2001} point out that in
suitably constrained distributional representations, distributional
similarity indicates substitutability in text. If two words $v$ and
$w$ are similar in their observed contexts, then $w$ can be
substituted for $v$ in texts. This can be written as an
inference rule $v \to w$, weighted by $\simfunc(\vec v, \vec w)$.

We use this same idea to project inference rules from vector space to logical
form through the lexical mapping function. If the lexical mapping function maps
the $n$-ary predicate $P$ to $\vec v$ and the $n$-ary predicate $Q$ to $ \vec w$,
and $\simfunc(\vec v, \vec w) = \eta$, then we obtain the weighted inference
rule $\forall x_1, \ldots, x_n[ P(x_1, \ldots, x_n) \to Q(x_1, \ldots x_n) ]$
with weight $\eta$. More generally, let \loglang be a logical language with
lexical mapping $\ell$ to a vector space $V$. Let \simfunc{} be the similarity
function on $V$. For all $Q \in \predsym\loglang$ and ${\cal Q} \subseteq
\predsym\loglang$, let $\zeta(Q, {\cal Q}) \subseteq {\cal Q}$. Then the
\emph{inference projection} for the predicate $P \in \predsym{\loglang}^n$ is
\begin{align*}
&\Pi_{\simfunc, \zeta, \ell}(P) = \{ (F, \eta) \mid \exists Q \in \zeta(P, \predsym{\loglang}^n)\:[ \\ 
& \hspace{125px} F = \forall x_1, \ldots, x_n[P(x_1, \ldots, x_n) \to Q(x_1, \ldots, x_n)], \\
& \hspace{125px} \eta = \simfunc\big(\ell(P), \ell(Q)\big) ~] \}
\end{align*}
That is, the inference projection for $P$ is the set of all weighted inference
rules $(F, \eta)$ predicted by the vector space that let us infer some
other predicate $Q$ from $P$. Additionally, we may have information on the 
inferences that we are willing to project that is not encoded in the vector 
space. For example we may only want to consider
predicates $Q$ that stand for paraphrases of $P$. For this reason, the
function $\zeta$ can be used to limit the predicates $Q$ considered for the right-hand
sides of rules. If $\zeta(P, \predsym\loglang^n) =
\predsym\loglang^n$, then a rule will
be generated for every $Q \in \predsym{\loglang}^n$. 


\subsection*{Addressing polysemy}
\index{polysemy}

When a word is polysemous, this affects the applicability of vector
space-based inference rules. Consider the rule $\forall e[fix(e) \to
correct(e)]$ (any fixing event is a correcting event): This rule
applies in contexts like ``fix a problem'', but not in contexts like
``fix the date''. We therefore need to take context into account when
considering inference rule applicability. We do this by computing
vector representations for word meaning in context, and predicting
rule applicability based on these context-specific vectors. We follow
the literature on vector space representations for word meaning in context
\citep{erk:emnlp2008,thater:acl2010,reisinger:naacl2010,dinu:emnlp2010,vandecruys:emnlp2011}
in assuming that a word's context-specific meaning is a function of its
out-of-context representation and the context. The context may consist
of a single item or multiple items, and (syntactic or semantic) relations to the
target word may also play a
role~\citep{erk:emnlp2008,thater:acl2010,vandecruys:emnlp2011}. 

We first define what we mean by a context. Given a vector space $V$ and a finite
set $R$ of semantic relations, the set \textit{$C(V, R)$ of contexts over $V$
and $R$} consists of all finite sets of pairs from $V \times R$.  That is, we
describe the context in which a target word occurs as a finite set of pairs
$(\vec v, r)$ of a context item $\vec v$ represented as a point in vector space,
and the relation $r$ between the context item and the target.
For a word $w$ in a context $c \in C(V, R)$, the context-specific meaning $\vec
w_c$ of $w$ is a function of the out-of-context vector $\vec w$ for $w$ and the
context $c$:
\[\vec w_c = \alpha(\vec w, c)\] The function $\alpha$ is a
\emph{contextualization function} with signature $\alpha: V \times C(V, R) \to
V$.

This definition of contextualization functions is similar to the framework of
\citet{mitchell:acl2008}, who define the meaning $\vec p$ of a two-word phrase
$p = vw$ as a function of the vectors for $v$ and $w$, and their syntactic
relation $r$ in the text: $\vec p = f(\vec v, \vec w, r, K)$,  where $f$ is some
function, and $K$ is background knowledge. However, we use contextualization
functions to compute the meaning of a word in context, rather than the meaning
of a phrase. We map predicate symbols to points in space, and predicate symbols
need to map to word meanings, not phrase meanings. Also, Mitchell and Lapata
only consider the case of two-word phrases, while we allow for arbitrary-size
contexts.


In existing approaches to computing word meaning in context, bag-of-words
representations or syntactic parses of the sentence context are used to compute
the contextualization. In contrast, we use the logical form representation,
through a function that maps a logic formula to a context in $C(V, R)$.  Given a
logical language \loglang, a vector space $V$, and set $R$ of semantic
relations, a \textit{context mapping} is a function that computes the context $c
\in C(V, R)$ of a predicate $P$ in a formula $G$ as \[c = \kappa(P, G)\] The
signature of a context mapping function is $\kappa:
\predsym{\loglang} \times \loglang \to C(V, R)$.

We can now compute a context-specific vector space representation $\vec{w}_{P,
G}$ for a predicate $P$ in a formula $G$ from the context-independent vector
$\ell(P)$ and the context $\kappa(P, G)$. It is \[\vec{w}_{P, G} =
\alpha\big(\ell(P), \kappa(P, G)\big)\]

To obtain an inference projection for $P$ that takes into account its context in
the formula $G$, we adapt the lexical mapping function. Given a lexical mapping
$\ell$, let $\ell_{[Q/\vec{v}]}$ be the function that is exactly like $\ell$
except that it maps $Q$ to $\vec v$. Let $\Pi_{\simfunc, \zeta, \ell}$ be an
inference projection for vector space $V$ and logical language $\loglang$, let
$\alpha$ be a contextualization function on $V$ and $R$, and $\kappa$ a context
mapping from \loglang to $C(V, R)$. Then the \textit{contextualized inference
projection} for predicate $P \in \predsym{\loglang}^n$ in formula $G \in
\loglang$ is \[ \Pi^G_{\simfunc, \zeta, \ell}(P) = \Pi_{\simfunc, \zeta,
  \ell_{[P/\alpha(\ell(P), \kappa(P, G))}}(P)
\] In this contextualized inference projection, any rule $\forall x_1,\ldots,x_n [P(
x_1,\ldots,x_n) \to Q(x_1,\ldots,x_n)]$ is weighted by similarity $\simfunc(\alpha(\ell(P), \kappa(P,
G)), \ell(Q))$ between the context-specific vector for $P$ and the vector for
$Q$. This follows common practice in vector space models of word meaning in
context of computing a context-specific representation of the target, but not
the paraphrase candidate. But if the paraphrase candidate is polysemous, it may
be useful to compute a representation for it that is also specific to the
sentence context at hand~\citep{erk:acl2010}.  We can do this by defining a
lexical mapping $\gamma^{P, G}$ specific to predicate $P$ and formula $G$ by
$\gamma^{P, G}(Q) = \alpha\big(\ell(Q), \kappa(P, G)\big)$. Then we can compute
the contextualized inference projection of $P$ as $\Pi^G_{\simfunc, \zeta,
\ell}(P) = \Pi_{\simfunc, \zeta, \gamma^{P, G}}(P)$.

In computational semantics, polysemy is mostly addressed by using multiple
predicates. For example, for the noun ``bank'' there would be predicates
bank$_1$, bank$_2$ to cover the financial and riverside senses of the word. In
contrast, we use a separate predicate for each word token, but these 
predicates are not associated with any particular fixed senses. Instead, we vary
the lexical mapping of a predicate based on the formula that it appears in: 
A predicate $P$ in
a formula $G$ is mapped to the vector $\alpha\big(\ell(P), \kappa(P, G)\big)$,
which depends on $G$. We make this change for two reasons. First, a system that
uses distinct predicates bank$_1$, bank$_2$ has to rely on an external word
sense disambiguation system that decides, during semantics construction, which
of the senses to use. In contrast, we determine lexical meaning based on the
overall semantic representation of a sentence, directly linking sentence
semantics and lexical semantics. Second, in the case of polysemy, the senses to
distinguish are not always that clear. For example, for a noun like ``onion'',
should the vegetable sense and the plant/bulb sense be
separate~\citep{krishnamurthy:chj2000}? Through the vector space model, we
can model word meaning in context without ever referring to distinct dictionary
senses~\citep{erk:gems2010}. 
But if we do not want to consider a fixed list of senses for a word
$w$, then we also cannot represent its meanings through a fixed list of predicates.
